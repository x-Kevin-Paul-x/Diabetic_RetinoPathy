{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7195874a",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ðŸ“¦ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc08725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Google Colab: True\n",
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3583845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q pytorch-lightning timm albumentations opencv-python-headless scikit-learn pandas tqdm matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90914ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸ Using device: cuda\n",
      "âš¡ cuDNN benchmark: ENABLED\n",
      "âš¡ TF32 mode: ENABLED (for Ampere GPUs)\n",
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Tuple, Union, Callable\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "\n",
    "# Model\n",
    "import timm\n",
    "\n",
    "# Augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# Optimization\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# GPU & Performance Optimizations\n",
    "# ============================================\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸ–¥ï¸ Using device: {DEVICE}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Enable cuDNN auto-tuner for faster training\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # Enable TF32 for Ampere GPUs (faster matmul)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(f\"âš¡ cuDNN benchmark: ENABLED\")\n",
    "    print(f\"âš¡ TF32 mode: ENABLED (for Ampere GPUs)\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Note: We keep deterministic=False for speed with benchmark=True\n",
    "    pl.seed_everything(seed, workers=True)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116e932",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ðŸ“ Data Loading (Kaggle API)\n",
    "\n",
    "Since VS Code's remote connection to Colab doesn't show the Google Drive auth popup, we'll download from Kaggle instead.\n",
    "\n",
    "**First time setup:**\n",
    "1. Go to [kaggle.com](https://kaggle.com) â†’ Your Profile â†’ Account â†’ Create API Token\n",
    "2. Download `kaggle.json`\n",
    "3. Upload it using the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0febf07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Kaggle API configured!\n",
      "   Username: kevinpaulg\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SETUP KAGGLE API KEY\n",
    "# ============================================\n",
    "# Your kaggle.json is at C:\\Users\\kevin\\.kaggle\\kaggle.json\n",
    "# Open that file and paste the contents below\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# PASTE YOUR KAGGLE CREDENTIALS HERE (open C:\\Users\\kevin\\.kaggle\\kaggle.json and copy contents)\n",
    "kaggle_credentials = {\"username\":\"kevinpaulg\",\"key\":\"\"}\n",
    "\n",
    "# Write to Colab\n",
    "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
    "    json.dump(kaggle_credentials, f)\n",
    "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
    "\n",
    "print(\"âœ… Kaggle API configured!\")\n",
    "print(f\"   Username: {kaggle_credentials.get('username', 'NOT SET')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "087144ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Downloading APTOS 2019 dataset from Kaggle...\n",
      "âš ï¸  This is a large dataset (~13GB) - please wait!\n",
      "==================================================\n",
      "^C\n",
      "\n",
      "âŒ Download failed! The zip file was not created.\n",
      "   Please try running this cell again.\n",
      "\n",
      "ðŸ“ Dataset contents:\n",
      "total 8\n",
      "drwxr-xr-x 2 root root 4096 Dec  2 12:40 .\n",
      "drwxr-xr-x 3 root root 4096 Dec  2 12:40 ..\n",
      "^C\n",
      "\n",
      "âŒ Download failed! The zip file was not created.\n",
      "   Please try running this cell again.\n",
      "\n",
      "ðŸ“ Dataset contents:\n",
      "total 8\n",
      "drwxr-xr-x 2 root root 4096 Dec  2 12:40 .\n",
      "drwxr-xr-x 3 root root 4096 Dec  2 12:40 ..\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DOWNLOAD APTOS DATASET FROM KAGGLE\n",
    "# ============================================\n",
    "# This downloads ~13GB of data - be patient!\n",
    "\n",
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"/content/data/aptos\", exist_ok=True)\n",
    "\n",
    "print(\"ðŸ“¥ Downloading APTOS 2019 dataset from Kaggle...\")\n",
    "print(\"âš ï¸  This is a large dataset (~13GB) - please wait!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if already downloaded\n",
    "zip_path = \"/content/data/aptos2019-blindness-detection.zip\"\n",
    "csv_exists = os.path.exists(\"/content/data/aptos/train.csv\")\n",
    "\n",
    "if csv_exists:\n",
    "    print(\"âœ… Dataset already downloaded and extracted!\")\n",
    "else:\n",
    "    # Download with verbose output\n",
    "    !kaggle competitions download -c aptos2019-blindness-detection -p /content/data\n",
    "    \n",
    "    # Check if download succeeded\n",
    "    if os.path.exists(zip_path):\n",
    "        print(\"\\nâœ… Download complete!\")\n",
    "        print(\"ðŸ“¦ Extracting dataset (this may take a minute)...\")\n",
    "        !unzip -q -o {zip_path} -d /content/data/aptos\n",
    "        print(\"âœ… Extraction complete!\")\n",
    "        \n",
    "        # Clean up zip to save space\n",
    "        !rm {zip_path}\n",
    "        print(\"ðŸ—‘ï¸ Cleaned up zip file to save disk space\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Download failed! The zip file was not created.\")\n",
    "        print(\"   Please try running this cell again.\")\n",
    "\n",
    "# Verify contents\n",
    "print(\"\\nðŸ“ Dataset contents:\")\n",
    "!ls -la /content/data/aptos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51d7b1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for: /content/drive/MyDrive/aptos2019-blindness-detection.zip\n",
      "âŒ Zip not found at: /content/drive/MyDrive/aptos2019-blindness-detection.zip\n",
      "\n",
      "ðŸ“‚ Let's find it. Contents of your Drive:\n",
      "ls: cannot access '/content/drive/MyDrive/': No such file or directory\n",
      "\n",
      "ðŸ’¡ Update DRIVE_ZIP path above to match your zip file location\n",
      "\n",
      "ðŸ’¡ Update DRIVE_ZIP path above to match your zip file location\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXTRACT ZIP FROM GOOGLE DRIVE\n",
    "# ============================================\n",
    "# Update this path to where your zip file is in Google Drive!\n",
    "\n",
    "import os\n",
    "\n",
    "# UPDATE THIS PATH to your actual zip location in Google Drive\n",
    "DRIVE_ZIP = \"/content/drive/MyDrive/aptos2019-blindness-detection.zip\"\n",
    "\n",
    "# Alternative common locations - uncomment the right one:\n",
    "# DRIVE_ZIP = \"/content/drive/MyDrive/Datasets/aptos2019-blindness-detection.zip\"\n",
    "# DRIVE_ZIP = \"/content/drive/MyDrive/ML/aptos2019-blindness-detection.zip\"\n",
    "\n",
    "print(f\"Looking for: {DRIVE_ZIP}\")\n",
    "\n",
    "if os.path.exists(DRIVE_ZIP):\n",
    "    print(\"âœ… Found zip file!\")\n",
    "    os.makedirs(\"/content/data/aptos\", exist_ok=True)\n",
    "    \n",
    "    print(\"ðŸ“¦ Extracting (this takes ~1 minute)...\")\n",
    "    !unzip -q -o \"{DRIVE_ZIP}\" -d /content/data/aptos\n",
    "    \n",
    "    print(\"âœ… Extraction complete!\")\n",
    "    !ls -la /content/data/aptos/\n",
    "else:\n",
    "    print(f\"âŒ Zip not found at: {DRIVE_ZIP}\")\n",
    "    print(\"\\nðŸ“‚ Let's find it. Contents of your Drive:\")\n",
    "    !ls -la /content/drive/MyDrive/ | head -20\n",
    "    print(\"\\nðŸ’¡ Update DRIVE_ZIP path above to match your zip file location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2790b6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Data Verification:\n",
      "âŒ Labels not found: /content/data/aptos/train.csv\n",
      "âŒ Raw images not found: /content/data/aptos/train_images\n",
      "â„¹ï¸  No processed/ folder yet (will be created)\n",
      "\n",
      "==================================================\n",
      "âš ï¸ Please check your data\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION & DATA LOADING\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Training configuration optimized for GPU performance.\"\"\"\n",
    "    # Paths - Data extracted from Google Drive\n",
    "    DATA_DIR = Path(\"/content/data/aptos\")\n",
    "    TRAIN_CSV = \"train.csv\"\n",
    "    TRAIN_IMAGES = \"train_images\"  # Raw images folder\n",
    "    PROCESSED_IMAGES = \"processed\"  # Preprocessed images (will be created)\n",
    "    CHECKPOINT_DIR = Path(\"/content/checkpoints\")\n",
    "    LOG_DIR = Path(\"/content/logs\")\n",
    "    \n",
    "    # Dataset\n",
    "    NUM_CLASSES = 5\n",
    "    CLASS_NAMES = [\"No DR\", \"Mild\", \"Moderate\", \"Severe\", \"Proliferative DR\"]\n",
    "    \n",
    "    # Model\n",
    "    BACKBONE = \"tf_efficientnet_b5_ns\"\n",
    "    IMAGE_SIZE = 456\n",
    "    POOLING = \"gem\"\n",
    "    HEAD_TYPE = \"regression\"\n",
    "    HEAD_DROPOUT = 0.5\n",
    "    \n",
    "    # Training - Optimized for GPU\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 16  # Adjust based on GPU (T4: 16, V100: 32)\n",
    "    ACCUMULATE_GRAD = 2\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    PATIENCE = 5\n",
    "    VAL_SPLIT = 0.2\n",
    "    \n",
    "    # Data Loading\n",
    "    NUM_WORKERS = 2  # Colab works better with 2\n",
    "    PREFETCH_FACTOR = 2\n",
    "    PERSISTENT_WORKERS = True\n",
    "    \n",
    "    # GPU Optimizations\n",
    "    PRECISION = \"16-mixed\" if torch.cuda.is_available() else 32\n",
    "    PIN_MEMORY = torch.cuda.is_available()\n",
    "    \n",
    "    # Preprocessing\n",
    "    BEN_GRAHAM_SIZE = 512\n",
    "    USE_PREPROCESSING = True\n",
    "    \n",
    "    # Seed\n",
    "    SEED = 42\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "config.CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "config.LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================\n",
    "# VERIFY DATA\n",
    "# ============================================\n",
    "csv_path = config.DATA_DIR / config.TRAIN_CSV\n",
    "raw_images_path = config.DATA_DIR / config.TRAIN_IMAGES\n",
    "\n",
    "print(\"ðŸ“Š Data Verification:\")\n",
    "\n",
    "# Load CSV\n",
    "if csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"âœ… Labels: {len(df)} samples\")\n",
    "    print(f\"   Classes: {df['diagnosis'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    print(f\"âŒ Labels not found: {csv_path}\")\n",
    "    df = None\n",
    "\n",
    "# Check raw images\n",
    "if raw_images_path.exists():\n",
    "    raw_files = list(raw_images_path.glob(\"*.png\")) + list(raw_images_path.glob(\"*.jpg\"))\n",
    "    print(f\"âœ… Raw images: {len(raw_files)} files in train_images/\")\n",
    "else:\n",
    "    print(f\"âŒ Raw images not found: {raw_images_path}\")\n",
    "\n",
    "# Check for processed (may not exist yet)\n",
    "processed_path = config.DATA_DIR / config.PROCESSED_IMAGES\n",
    "if processed_path.exists():\n",
    "    proc_files = list(processed_path.glob(\"*.png\"))\n",
    "    print(f\"âœ… Processed images: {len(proc_files)} files\")\n",
    "else:\n",
    "    print(f\"â„¹ï¸  No processed/ folder yet (will be created)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸŽ‰ Data ready for preprocessing!\" if df is not None else \"âš ï¸ Please check your data\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5d44c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Configuration:\n",
      "  - Environment: Google Colab\n",
      "  - Base Directory: /content\n",
      "  - Data Directory: /content/data/aptos\n",
      "  - Backbone: tf_efficientnet_b5_ns\n",
      "  - Image Size: 456\n",
      "  - Batch Size: 16 (effective: 32)\n",
      "  - Epochs: 30\n",
      "  - Learning Rate: 0.0001\n",
      "  - Precision: 16-mixed\n",
      "  - Num Workers: 4\n",
      "  - Pin Memory: True\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "import os\n",
    "\n",
    "# Determine base directory based on environment\n",
    "if IN_COLAB:\n",
    "    # In Colab, data should be at /content/data/aptos after running data setup cell\n",
    "    _base_dir = Path(\"/content\")\n",
    "else:\n",
    "    # Local: handle running from notebooks/ or project root\n",
    "    _notebook_dir = Path(os.getcwd())\n",
    "    if _notebook_dir.name == \"notebooks\":\n",
    "        _base_dir = _notebook_dir.parent\n",
    "    else:\n",
    "        _base_dir = _notebook_dir\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Training configuration optimized for GPU performance.\"\"\"\n",
    "    # Paths - Dynamically resolved based on environment\n",
    "    DATA_DIR = _base_dir / \"data\" / \"aptos\"\n",
    "    TRAIN_CSV = \"train.csv\"\n",
    "    TRAIN_IMAGES = \"train_images\"\n",
    "    PROCESSED_IMAGES = \"processed\"  # Your preprocessed images folder\n",
    "    CHECKPOINT_DIR = _base_dir / \"checkpoints\"\n",
    "    LOG_DIR = _base_dir / \"logs\"\n",
    "    \n",
    "    # Dataset\n",
    "    NUM_CLASSES = 5\n",
    "    CLASS_NAMES = [\"No DR\", \"Mild\", \"Moderate\", \"Severe\", \"Proliferative DR\"]\n",
    "    \n",
    "    # Model\n",
    "    BACKBONE = \"tf_efficientnet_b5_ns\"  # Noisy Student pretrained\n",
    "    IMAGE_SIZE = 456\n",
    "    POOLING = \"gem\"  # Generalized Mean Pooling\n",
    "    HEAD_TYPE = \"regression\"\n",
    "    HEAD_DROPOUT = 0.5\n",
    "    \n",
    "    # Training - Optimized for GPU\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 16  # Adjust based on GPU memory (V100: 32, T4: 16, 3090: 24)\n",
    "    ACCUMULATE_GRAD = 2  # Effective batch size = BATCH_SIZE * ACCUMULATE_GRAD\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    PATIENCE = 5\n",
    "    VAL_SPLIT = 0.2\n",
    "    \n",
    "    # Data Loading - Optimized for fast I/O\n",
    "    NUM_WORKERS = 4 if torch.cuda.is_available() else 2  # More workers for GPU\n",
    "    PREFETCH_FACTOR = 2  # Prefetch batches in parallel\n",
    "    PERSISTENT_WORKERS = True  # Keep workers alive between epochs\n",
    "    \n",
    "    # GPU Optimizations\n",
    "    PRECISION = \"16-mixed\" if torch.cuda.is_available() else 32  # Mixed precision\n",
    "    PIN_MEMORY = torch.cuda.is_available()  # Pin memory for faster GPU transfer\n",
    "    \n",
    "    # Preprocessing\n",
    "    BEN_GRAHAM_SIZE = 512\n",
    "    USE_PREPROCESSING = True  # We have preprocessed images\n",
    "    \n",
    "    # Seed\n",
    "    SEED = 42\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "config.DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "config.CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "config.LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ðŸ“‹ Configuration:\")\n",
    "print(f\"  - Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"  - Base Directory: {_base_dir}\")\n",
    "print(f\"  - Data Directory: {config.DATA_DIR}\")\n",
    "print(f\"  - Backbone: {config.BACKBONE}\")\n",
    "print(f\"  - Image Size: {config.IMAGE_SIZE}\")\n",
    "print(f\"  - Batch Size: {config.BATCH_SIZE} (effective: {config.BATCH_SIZE * config.ACCUMULATE_GRAD})\")\n",
    "print(f\"  - Epochs: {config.EPOCHS}\")\n",
    "print(f\"  - Learning Rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  - Precision: {config.PRECISION}\")\n",
    "print(f\"  - Num Workers: {config.NUM_WORKERS}\")\n",
    "print(f\"  - Pin Memory: {config.PIN_MEMORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd2c37ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ DataFrame 'df' is not loaded!\n",
      "   Please run the data loading cells first (Cells 6-8)\n",
      "\n",
      "   To debug, run this:\n",
      "   !ls -la /content/data/aptos/\n"
     ]
    }
   ],
   "source": [
    "# Visualize class distribution\n",
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Count plot\n",
    "    class_counts = df['diagnosis'].value_counts().sort_index()\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.8, 0.2, 5))\n",
    "    \n",
    "    axes[0].bar(range(5), class_counts.values, color=colors)\n",
    "    axes[0].set_xticks(range(5))\n",
    "    axes[0].set_xticklabels(config.CLASS_NAMES, rotation=45, ha='right')\n",
    "    axes[0].set_xlabel('DR Grade')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Class Distribution')\n",
    "    \n",
    "    for i, v in enumerate(class_counts.values):\n",
    "        axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(class_counts.values, labels=config.CLASS_NAMES, autopct='%1.1f%%', \n",
    "                colors=colors, explode=[0.05]*5)\n",
    "    axes[1].set_title('Class Distribution (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š Class Distribution:\")\n",
    "    for i, name in enumerate(config.CLASS_NAMES):\n",
    "        count = class_counts.get(i, 0)\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {i} - {name}: {count} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"âš ï¸ DataFrame 'df' is not loaded!\")\n",
    "    print(\"   Please run the data loading cells first (Cells 6-8)\")\n",
    "    print(\"\\n   To debug, run this:\")\n",
    "    print(\"   !ls -la /content/data/aptos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec302ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /content\n",
      "Data directory: /content/data/aptos\n",
      "Train images directory: /content/data/aptos/train_images\n",
      "âŒ Image directory not found at /content/data/aptos/train_images\n",
      "   Please check if the data is in the correct location.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# VERIFY RAW DATA IS LOADED\n",
    "# ============================================\n",
    "import os\n",
    "\n",
    "# Get current directory and resolve paths\n",
    "notebook_dir = Path(os.getcwd())\n",
    "print(f\"Current working directory: {notebook_dir}\")\n",
    "\n",
    "# If running from notebooks folder, adjust the base path\n",
    "if notebook_dir.name == \"notebooks\":\n",
    "    base_dir = notebook_dir.parent\n",
    "else:\n",
    "    base_dir = notebook_dir\n",
    "\n",
    "# Update config paths to absolute\n",
    "config.DATA_DIR = base_dir / \"data\" / \"aptos\"\n",
    "config.CHECKPOINT_DIR = base_dir / \"checkpoints\"\n",
    "config.LOG_DIR = base_dir / \"logs\"\n",
    "\n",
    "# Define paths\n",
    "train_csv_path = config.DATA_DIR / config.TRAIN_CSV\n",
    "train_images_dir = config.DATA_DIR / config.TRAIN_IMAGES  # Raw images\n",
    "processed_dir = config.DATA_DIR / config.PROCESSED_IMAGES  # Will be created\n",
    "\n",
    "print(f\"\\nðŸ“ Data Paths:\")\n",
    "print(f\"   - Data directory: {config.DATA_DIR}\")\n",
    "print(f\"   - Labels file: {train_csv_path}\")\n",
    "print(f\"   - Raw images: {train_images_dir}\")\n",
    "print(f\"   - Processed output: {processed_dir}\")\n",
    "\n",
    "# Check raw images (this is what we need!)\n",
    "print(f\"\\nðŸ“Š Raw Data Check:\")\n",
    "if train_images_dir.exists():\n",
    "    raw_images = list(train_images_dir.glob(\"*.png\")) + list(train_images_dir.glob(\"*.jpg\"))\n",
    "    print(f\"   âœ… Found {len(raw_images)} raw images in train_images/\")\n",
    "    \n",
    "    # Show sample raw image\n",
    "    if raw_images:\n",
    "        sample_img_path = raw_images[0]\n",
    "        sample_img = cv2.imread(str(sample_img_path))\n",
    "        sample_img = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(sample_img)\n",
    "        plt.title(f\"Sample RAW Image: {sample_img_path.name}\\nShape: {sample_img.shape}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Preview what preprocessing will do\n",
    "        preprocessor = BenGrahamPreprocessor(output_size=config.BEN_GRAHAM_SIZE)\n",
    "        processed_preview = preprocessor(cv2.cvtColor(sample_img, cv2.COLOR_RGB2BGR))\n",
    "        processed_preview = cv2.cvtColor(processed_preview, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(processed_preview)\n",
    "        plt.title(f\"After Ben Graham Preprocessing\\nShape: {processed_preview.shape}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.suptitle(\"Raw â†’ Processed Preview\", fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"   âŒ Raw images not found at {train_images_dir}\")\n",
    "    print(f\"      Please run the data acquisition cell above!\")\n",
    "\n",
    "# Check if processed folder already exists (from previous run)\n",
    "if processed_dir.exists():\n",
    "    processed_images = list(processed_dir.glob(\"*.png\"))\n",
    "    print(f\"   â„¹ï¸ Found {len(processed_images)} already-processed images in processed/\")\n",
    "else:\n",
    "    print(f\"   â„¹ï¸ No processed/ folder yet - will be created during preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70f1d7",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ðŸ”§ Ben Graham Preprocessing (Raw â†’ Processed)\n",
    "\n",
    "**This section transforms raw fundus images into preprocessed images for training.**\n",
    "\n",
    "The Ben Graham preprocessing technique:\n",
    "1. **Crops** the fundus image to remove black borders\n",
    "2. **Subtracts local Gaussian blur** for luminosity normalization  \n",
    "3. **Applies circular mask** to remove edge artifacts\n",
    "\n",
    "This significantly improves model performance by normalizing image quality across different cameras/settings.\n",
    "\n",
    "**Input:** `train_images/` (raw images)  \n",
    "**Output:** `processed/` (preprocessed images ready for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c0dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BenGrahamPreprocessor defined!\n"
     ]
    }
   ],
   "source": [
    "class BenGrahamPreprocessor:\n",
    "    \"\"\"\n",
    "    Ben Graham preprocessing for fundus images.\n",
    "    \n",
    "    Formula: I_processed = Î± * I_original + Î² * G(I_original, Ïƒ) + Î³\n",
    "    Default: Î±=4, Î²=-4, Î³=128 (acts as high-pass filter)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size: int = 512,\n",
    "        alpha: float = 4.0,\n",
    "        beta: float = -4.0,\n",
    "        gamma: float = 128.0,\n",
    "        sigma_ratio: float = 0.1,\n",
    "        crop_black_margin: bool = True,\n",
    "        apply_circular_mask: bool = True,\n",
    "    ):\n",
    "        self.output_size = output_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.sigma_ratio = sigma_ratio\n",
    "        self.crop_black_margin = crop_black_margin\n",
    "        self.apply_circular_mask = apply_circular_mask\n",
    "        \n",
    "        sigma = int(output_size * sigma_ratio)\n",
    "        self.kernel_size = sigma * 2 + 1\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def _find_eye_region(self, image: np.ndarray) -> Tuple[int, int, int, int]:\n",
    "        \"\"\"Find bounding box of the eye region.\"\"\"\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if not contours:\n",
    "            return 0, 0, image.shape[1], image.shape[0]\n",
    "        \n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        return cv2.boundingRect(largest_contour)\n",
    "    \n",
    "    def _crop_to_square(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Crop image to square centered on eye.\"\"\"\n",
    "        if not self.crop_black_margin:\n",
    "            h, w = image.shape[:2]\n",
    "            min_dim = min(h, w)\n",
    "            start_x, start_y = (w - min_dim) // 2, (h - min_dim) // 2\n",
    "            return image[start_y:start_y + min_dim, start_x:start_x + min_dim]\n",
    "        \n",
    "        x, y, w, h = self._find_eye_region(image)\n",
    "        center_x, center_y = x + w // 2, y + h // 2\n",
    "        \n",
    "        size = int(max(w, h) * 1.05)\n",
    "        half_size = size // 2\n",
    "        \n",
    "        img_h, img_w = image.shape[:2]\n",
    "        x1 = max(0, center_x - half_size)\n",
    "        y1 = max(0, center_y - half_size)\n",
    "        x2 = min(img_w, center_x + half_size)\n",
    "        y2 = min(img_h, center_y + half_size)\n",
    "        \n",
    "        cropped = image[y1:y2, x1:x2]\n",
    "        \n",
    "        crop_h, crop_w = cropped.shape[:2]\n",
    "        if crop_h != crop_w:\n",
    "            max_side = max(crop_h, crop_w)\n",
    "            square = np.zeros((max_side, max_side, 3), dtype=np.uint8)\n",
    "            y_offset = (max_side - crop_h) // 2\n",
    "            x_offset = (max_side - crop_w) // 2\n",
    "            square[y_offset:y_offset + crop_h, x_offset:x_offset + crop_w] = cropped\n",
    "            cropped = square\n",
    "        \n",
    "        return cropped\n",
    "    \n",
    "    def _apply_gaussian_filter(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply Ben Graham Gaussian blur subtraction.\"\"\"\n",
    "        blurred = cv2.GaussianBlur(image, (self.kernel_size, self.kernel_size), self.sigma)\n",
    "        processed = cv2.addWeighted(image, self.alpha, blurred, self.beta, self.gamma)\n",
    "        return np.clip(processed, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    def _apply_circular_mask_fn(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply circular mask.\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        radius = min(center) - 2\n",
    "        \n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.circle(mask, center, radius, 255, -1)\n",
    "        \n",
    "        result = image.copy()\n",
    "        result[mask == 0] = 0\n",
    "        return result\n",
    "    \n",
    "    def __call__(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply full preprocessing pipeline.\"\"\"\n",
    "        cropped = self._crop_to_square(image)\n",
    "        resized = cv2.resize(cropped, (self.output_size, self.output_size), interpolation=cv2.INTER_LINEAR)\n",
    "        processed = self._apply_gaussian_filter(resized)\n",
    "        \n",
    "        if self.apply_circular_mask:\n",
    "            processed = self._apply_circular_mask_fn(processed)\n",
    "        \n",
    "        return processed\n",
    "\n",
    "print(\"âœ… BenGrahamPreprocessor defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate preprocessing on sample images\n",
    "if 'df' in dir() and train_images_dir.exists():\n",
    "    preprocessor = BenGrahamPreprocessor(output_size=config.BEN_GRAHAM_SIZE)\n",
    "    \n",
    "    # Get one sample per class\n",
    "    fig, axes = plt.subplots(5, 3, figsize=(15, 25))\n",
    "    \n",
    "    for class_idx in range(5):\n",
    "        # Get sample from this class\n",
    "        sample = df[df['diagnosis'] == class_idx].iloc[0]\n",
    "        img_name = sample['id_code']\n",
    "        \n",
    "        # Find image file\n",
    "        img_path = None\n",
    "        for ext in ['.png', '.jpg', '.jpeg']:\n",
    "            potential_path = train_images_dir / f\"{img_name}{ext}\"\n",
    "            if potential_path.exists():\n",
    "                img_path = potential_path\n",
    "                break\n",
    "        \n",
    "        if img_path:\n",
    "            # Load and process\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            processed = preprocessor(img)\n",
    "            processed_rgb = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Original\n",
    "            axes[class_idx, 0].imshow(img_rgb)\n",
    "            axes[class_idx, 0].set_title(f\"Original - {config.CLASS_NAMES[class_idx]}\")\n",
    "            axes[class_idx, 0].axis('off')\n",
    "            \n",
    "            # Processed\n",
    "            axes[class_idx, 1].imshow(processed_rgb)\n",
    "            axes[class_idx, 1].set_title(f\"Ben Graham Processed\")\n",
    "            axes[class_idx, 1].axis('off')\n",
    "            \n",
    "            # Histogram\n",
    "            for c, color in enumerate(['red', 'green', 'blue']):\n",
    "                axes[class_idx, 2].hist(processed_rgb[:,:,c].ravel(), bins=50, \n",
    "                                        alpha=0.5, color=color, label=color.upper())\n",
    "            axes[class_idx, 2].set_title('Color Histogram')\n",
    "            axes[class_idx, 2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\nâœ… Preprocessing demonstration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e816a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ï¸ Skipping batch preprocessing (set RUN_PREPROCESSING=True to run)\n",
      "   Images will be preprocessed on-the-fly during training.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PREPROCESS ALL RAW IMAGES â†’ PROCESSED FOLDER\n",
    "# ============================================\n",
    "# This is the KEY step: Convert raw train_images/ to processed/\n",
    "\n",
    "def preprocess_all_images(df, input_dir, output_dir, preprocessor, overwrite=False):\n",
    "    \"\"\"\n",
    "    Preprocess all raw images and save to processed folder.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'id_code' column\n",
    "        input_dir: Path to raw images (train_images/)\n",
    "        output_dir: Path to save processed images (processed/)\n",
    "        preprocessor: BenGrahamPreprocessor instance\n",
    "        overwrite: If True, reprocess even if output exists\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    success_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"ðŸ”„ Preprocessing raw images\"):\n",
    "        img_name = row['id_code']\n",
    "        output_path = output_dir / f\"{img_name}.png\"\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if output_path.exists() and not overwrite:\n",
    "            skipped_count += 1\n",
    "            success_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Find input image (raw)\n",
    "        img_path = None\n",
    "        for ext in ['.png', '.jpg', '.jpeg']:\n",
    "            potential_path = input_dir / f\"{img_name}{ext}\"\n",
    "            if potential_path.exists():\n",
    "                img_path = potential_path\n",
    "                break\n",
    "        \n",
    "        if img_path is None:\n",
    "            print(f\"âš ï¸ Warning: Raw image not found for {img_name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            processed = preprocessor(img)\n",
    "            cv2.imwrite(str(output_path), processed)\n",
    "            success_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {img_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Preprocessing complete!\")\n",
    "    print(f\"   - Processed: {success_count - skipped_count} new images\")\n",
    "    print(f\"   - Skipped (already done): {skipped_count} images\")\n",
    "    print(f\"   - Total ready: {success_count}/{len(df)} images\")\n",
    "    return success_count\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CHECK IF PREPROCESSING IS NEEDED\n",
    "# ============================================\n",
    "input_dir = config.DATA_DIR / config.TRAIN_IMAGES  # Raw images\n",
    "output_dir = config.DATA_DIR / config.PROCESSED_IMAGES  # Processed output\n",
    "\n",
    "# Count existing processed images\n",
    "if output_dir.exists():\n",
    "    processed_count = len(list(output_dir.glob(\"*.png\")))\n",
    "else:\n",
    "    processed_count = 0\n",
    "\n",
    "# Count raw images\n",
    "if input_dir.exists():\n",
    "    raw_count = len(list(input_dir.glob(\"*.png\"))) + len(list(input_dir.glob(\"*.jpg\")))\n",
    "else:\n",
    "    raw_count = 0\n",
    "\n",
    "print(f\"ðŸ“Š Preprocessing Status:\")\n",
    "print(f\"   - Raw images (train_images/): {raw_count}\")\n",
    "print(f\"   - Processed images (processed/): {processed_count}\")\n",
    "\n",
    "if 'df' in dir():\n",
    "    total_needed = len(df)\n",
    "    if processed_count >= total_needed:\n",
    "        print(f\"\\nâœ… All {total_needed} images already preprocessed! Skipping...\")\n",
    "        RUN_PREPROCESSING = False\n",
    "    elif processed_count > 0:\n",
    "        print(f\"\\nâš ï¸ Partial preprocessing: {processed_count}/{total_needed} done\")\n",
    "        print(\"   Set RUN_PREPROCESSING = True to complete preprocessing\")\n",
    "        RUN_PREPROCESSING = True  # Will only process missing images\n",
    "    else:\n",
    "        print(f\"\\nðŸ”„ Need to preprocess {total_needed} images\")\n",
    "        RUN_PREPROCESSING = True\n",
    "else:\n",
    "    print(\"\\nâš ï¸ DataFrame 'df' not found. Load train.csv first!\")\n",
    "    RUN_PREPROCESSING = False\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# RUN PREPROCESSING (if needed)\n",
    "# ============================================\n",
    "if RUN_PREPROCESSING and 'df' in dir():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ”§ Starting Ben Graham Preprocessing...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    preprocessor = BenGrahamPreprocessor(output_size=config.BEN_GRAHAM_SIZE)\n",
    "    \n",
    "    preprocess_all_images(\n",
    "        df=df,\n",
    "        input_dir=input_dir,\n",
    "        output_dir=output_dir,\n",
    "        preprocessor=preprocessor,\n",
    "        overwrite=False  # Skip already processed images\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸ“ Processed images saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01319d6c",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Data Pipeline Complete!\n",
    "\n",
    "**What happened:**\n",
    "1. âœ… **Raw images loaded** from `train_images/`\n",
    "2. âœ… **Ben Graham preprocessing applied** â†’ saved to `processed/`\n",
    "3. âœ… **Ready for training** with preprocessed images\n",
    "\n",
    "**Data Flow Summary:**\n",
    "```\n",
    "train_images/ (raw) â†’ Ben Graham â†’ processed/ (ready for training)\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "- Model architecture definition\n",
    "- Data augmentation and loaders\n",
    "- Training loop\n",
    "- Evaluation and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“Š DATA STATUS SUMMARY\n",
      "============================================================\n",
      "âŒ Training CSV not loaded\n",
      "âŒ Image directory not found\n",
      "â„¹ï¸ No preprocessed images (will process on-the-fly)\n",
      "============================================================\n",
      "\n",
      "ðŸš€ Ready for training! Run the next cells to continue.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DATA PIPELINE SUMMARY\n",
    "# ============================================\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š DATA PIPELINE STATUS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Check CSV\n",
    "if 'df' in dir():\n",
    "    print(f\"\\n1ï¸âƒ£ Labels (train.csv):\")\n",
    "    print(f\"   âœ… Loaded: {len(df)} samples\")\n",
    "    print(f\"   Classes: {df['diagnosis'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    print(f\"\\n1ï¸âƒ£ Labels: âŒ Not loaded\")\n",
    "\n",
    "# 2. Check raw images\n",
    "raw_dir = config.DATA_DIR / config.TRAIN_IMAGES\n",
    "if raw_dir.exists():\n",
    "    raw_count = len(list(raw_dir.glob(\"*.png\"))) + len(list(raw_dir.glob(\"*.jpg\")))\n",
    "    print(f\"\\n2ï¸âƒ£ Raw Images (train_images/):\")\n",
    "    print(f\"   âœ… Found: {raw_count} images\")\n",
    "else:\n",
    "    raw_count = 0\n",
    "    print(f\"\\n2ï¸âƒ£ Raw Images: âŒ Not found at {raw_dir}\")\n",
    "\n",
    "# 3. Check processed images\n",
    "proc_dir = config.DATA_DIR / config.PROCESSED_IMAGES\n",
    "if proc_dir.exists():\n",
    "    proc_count = len(list(proc_dir.glob(\"*.png\")))\n",
    "    print(f\"\\n3ï¸âƒ£ Processed Images (processed/):\")\n",
    "    print(f\"   âœ… Ready: {proc_count} images\")\n",
    "    \n",
    "    if 'df' in dir() and proc_count >= len(df):\n",
    "        print(f\"   âœ… All images preprocessed!\")\n",
    "    elif 'df' in dir():\n",
    "        print(f\"   âš ï¸ Missing: {len(df) - proc_count} images\")\n",
    "else:\n",
    "    print(f\"\\n3ï¸âƒ£ Processed Images: âŒ Not created yet\")\n",
    "    print(f\"   Run preprocessing cell above!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ Data pipeline ready! Continue to Model Architecture â†’\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8557bf1",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ðŸ—ï¸ Model Architecture\n",
    "\n",
    "The model consists of:\n",
    "- **Backbone**: EfficientNet-B5 with Noisy Student pretraining\n",
    "- **Pooling**: Generalized Mean Pooling (GeM) - learnable pooling between avg and max\n",
    "- **Head**: Regression head for ordinal output (0-4 scale)\n",
    "\n",
    "This architecture follows the winning solution from APTOS 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b266f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GeM Pooling and Regression Head defined!\n"
     ]
    }
   ],
   "source": [
    "# Generalized Mean Pooling (GeM)\n",
    "class GeM(nn.Module):\n",
    "    \"\"\"\n",
    "    Generalized Mean Pooling.\n",
    "    As p â†’ 1: average pooling\n",
    "    As p â†’ âˆž: max pooling\n",
    "    Default p=3 is a good balance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p: float = 3.0, eps: float = 1e-6, trainable: bool = True):\n",
    "        super().__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1) * p) if trainable else torch.tensor([p])\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.clamp(min=self.eps)\n",
    "        pooled = F.adaptive_avg_pool2d(x.pow(self.p), 1)\n",
    "        return pooled.pow(1.0 / self.p).flatten(1)\n",
    "\n",
    "\n",
    "# Model Heads\n",
    "class RegressionHead(nn.Module):\n",
    "    \"\"\"Regression head for ordinal output.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, hidden_dims: List[int] = [512], dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = in_features\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.head = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.xavier_uniform_(self.head[-1].weight)\n",
    "        nn.init.zeros_(self.head[-1].bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.head(x).squeeze(-1)\n",
    "\n",
    "\n",
    "print(\"âœ… GeM Pooling and Regression Head defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Testing model creation...\n",
      "Model created: efficientnet_b0\n",
      "  - Feature dimension: 1280\n",
      "  - Pooling: gem\n",
      "  - Head: Regression with dims [512]\n",
      "âœ… Test output shape: torch.Size([2])\n",
      "Model created: efficientnet_b0\n",
      "  - Feature dimension: 1280\n",
      "  - Pooling: gem\n",
      "  - Head: Regression with dims [512]\n",
      "âœ… Test output shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Complete DR Model\n",
    "class DRModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Diabetic Retinopathy Detection Model.\n",
    "    \n",
    "    Architecture:\n",
    "    - EfficientNet backbone (pretrained)\n",
    "    - GeM pooling\n",
    "    - Regression head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str = \"tf_efficientnet_b5_ns\",\n",
    "        pretrained: bool = True,\n",
    "        pooling: str = \"gem\",\n",
    "        head_dropout: float = 0.5,\n",
    "        hidden_dims: List[int] = [512],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create backbone without classifier\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,\n",
    "            global_pool=\"\",\n",
    "        )\n",
    "        \n",
    "        # Get feature dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            features = self.backbone(dummy)\n",
    "            self.num_features = features.shape[1]\n",
    "        \n",
    "        # Pooling\n",
    "        if pooling == \"gem\":\n",
    "            self.pool = GeM(p=3.0, trainable=True)\n",
    "        else:\n",
    "            self.pool = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        \n",
    "        # Head\n",
    "        self.head = RegressionHead(\n",
    "            in_features=self.num_features,\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout=head_dropout,\n",
    "        )\n",
    "        \n",
    "        print(f\"Model created: {backbone_name}\")\n",
    "        print(f\"  - Feature dimension: {self.num_features}\")\n",
    "        print(f\"  - Pooling: {pooling}\")\n",
    "        print(f\"  - Head: Regression with dims {hidden_dims}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.backbone(x)\n",
    "        pooled = self.pool(features)\n",
    "        return self.head(pooled)\n",
    "    \n",
    "    def get_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get features before head (for visualization).\"\"\"\n",
    "        features = self.backbone(x)\n",
    "        return self.pool(features)\n",
    "\n",
    "\n",
    "# Test model creation\n",
    "print(\"\\nðŸ§ª Testing model creation...\")\n",
    "test_model = DRModel(backbone_name=\"efficientnet_b0\", pretrained=True)  # Small model for test\n",
    "test_input = torch.randn(2, 3, 224, 224)\n",
    "test_output = test_model(test_input)\n",
    "print(f\"âœ… Test output shape: {test_output.shape}\")\n",
    "del test_model, test_input, test_output\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6d7eb",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ðŸ“¦ Dataset & Data Augmentation\n",
    "\n",
    "**The dataset loads preprocessed images from `processed/` folder** (created in Section 3).\n",
    "\n",
    "If preprocessed images are not found, it will apply Ben Graham preprocessing on-the-fly (slower).\n",
    "\n",
    "**Augmentations** (designed for retinal fundus images):\n",
    "- **Geometric**: Rotation (360Â°), flips - fundus images are rotationally invariant\n",
    "- **Photometric**: Brightness/contrast adjustments (moderate to preserve pathology colors)\n",
    "- **Dropout**: Coarse dropout for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962dc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transforms defined with optimized interpolation!\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation Transforms - Optimized for speed\n",
    "def get_train_transforms(image_size: int = 456) -> A.Compose:\n",
    "    \"\"\"Training augmentations for fundus images - optimized for GPU training.\"\"\"\n",
    "    return A.Compose([\n",
    "        # Use INTER_LINEAR (faster than default INTER_AREA for downscaling)\n",
    "        A.Resize(image_size, image_size, interpolation=cv2.INTER_LINEAR),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=360, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=0, \n",
    "                 interpolation=cv2.INTER_LINEAR),\n",
    "        A.RandomScale(scale_limit=0.2, p=0.5, interpolation=cv2.INTER_LINEAR),\n",
    "        A.PadIfNeeded(min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "        A.CenterCrop(height=image_size, width=image_size),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=20, p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "        A.CoarseDropout(num_holes_range=(1, 8), hole_height_range=(20, 50), \n",
    "                        hole_width_range=(20, 50), fill=0, p=0.3),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms(image_size: int = 456) -> A.Compose:\n",
    "    \"\"\"Validation transforms (no augmentation) - optimized for speed.\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(image_size, image_size, interpolation=cv2.INTER_LINEAR),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "print(\"âœ… Transforms defined with optimized interpolation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a38966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… APTOSDataset defined with optimized path caching!\n"
     ]
    }
   ],
   "source": [
    "# Dataset Class - Optimized for fast loading\n",
    "class APTOSDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for APTOS 2019 - Optimized for GPU training.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: pd.DataFrame,\n",
    "        image_dir: Path,\n",
    "        transform: Optional[Callable] = None,\n",
    "        preprocessor: Optional[BenGrahamPreprocessor] = None,\n",
    "        use_preprocessed: bool = False,\n",
    "        preprocessed_dir: Optional[Path] = None,\n",
    "    ):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "        self.preprocessor = preprocessor\n",
    "        self.use_preprocessed = use_preprocessed\n",
    "        self.preprocessed_dir = Path(preprocessed_dir) if preprocessed_dir else None\n",
    "        \n",
    "        self.labels = self.df['diagnosis'].values.astype(np.float32)  # Pre-cast to float32\n",
    "        self.image_ids = self.df['id_code'].values\n",
    "        \n",
    "        # Pre-compute image paths for faster access\n",
    "        self.image_paths = self._precompute_paths()\n",
    "    \n",
    "    def _precompute_paths(self) -> List[Path]:\n",
    "        \"\"\"Pre-compute all image paths to avoid repeated file system checks.\"\"\"\n",
    "        paths = []\n",
    "        for image_id in self.image_ids:\n",
    "            if self.use_preprocessed and self.preprocessed_dir:\n",
    "                path = self.preprocessed_dir / f\"{image_id}.png\"\n",
    "            else:\n",
    "                path = None\n",
    "                for ext in ['.png', '.jpg', '.jpeg']:\n",
    "                    potential_path = self.image_dir / f\"{image_id}{ext}\"\n",
    "                    if potential_path.exists():\n",
    "                        path = potential_path\n",
    "                        break\n",
    "                if path is None:\n",
    "                    path = self.image_dir / f\"{image_id}.png\"  # Default\n",
    "            paths.append(path)\n",
    "        return paths\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image using cv2 (faster than PIL for large images)\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not load image: {image_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply Ben Graham preprocessing if not using preprocessed images\n",
    "        if self.preprocessor and not self.use_preprocessed:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            image = self.preprocessor(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        else:\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'target': torch.tensor(label, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ… APTOSDataset defined with optimized path caching!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6e7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… APTOSDataModule defined with GPU optimizations!\n"
     ]
    }
   ],
   "source": [
    "# DataModule for PyTorch Lightning - Optimized for GPU Training\n",
    "class APTOSDataModule(pl.LightningDataModule):\n",
    "    \"\"\"PyTorch Lightning DataModule for APTOS dataset - Optimized for fast GPU training.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path,\n",
    "        train_csv: str,\n",
    "        image_dir: str,\n",
    "        processed_dir: Optional[str] = None,\n",
    "        batch_size: int = 16,\n",
    "        num_workers: int = 4,\n",
    "        image_size: int = 456,\n",
    "        val_split: float = 0.2,\n",
    "        use_weighted_sampler: bool = True,\n",
    "        use_preprocessing: bool = True,\n",
    "        seed: int = 42,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = True,\n",
    "        prefetch_factor: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.train_csv = train_csv\n",
    "        self.image_dir = image_dir\n",
    "        self.processed_dir = processed_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.image_size = image_size\n",
    "        self.val_split = val_split\n",
    "        self.use_weighted_sampler = use_weighted_sampler\n",
    "        self.use_preprocessing = use_preprocessing\n",
    "        self.seed = seed\n",
    "        \n",
    "        # GPU optimization settings\n",
    "        self.pin_memory = pin_memory and torch.cuda.is_available()\n",
    "        self.persistent_workers = persistent_workers and (num_workers > 0)\n",
    "        self.prefetch_factor = prefetch_factor if num_workers > 0 else None\n",
    "        \n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.class_weights = None\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(self.data_dir / self.train_csv)\n",
    "        \n",
    "        # Train/val split\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            np.arange(len(df)),\n",
    "            test_size=self.val_split,\n",
    "            stratify=df['diagnosis'],\n",
    "            random_state=self.seed\n",
    "        )\n",
    "        \n",
    "        self.train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        self.val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        # Calculate class weights\n",
    "        train_labels = self.train_df['diagnosis'].values\n",
    "        class_counts = np.bincount(train_labels, minlength=5)\n",
    "        self.class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "        \n",
    "        # Preprocessor\n",
    "        preprocessor = BenGrahamPreprocessor(output_size=512) if self.use_preprocessing else None\n",
    "        \n",
    "        # Check for preprocessed images\n",
    "        use_preprocessed = False\n",
    "        preprocessed_path = None\n",
    "        if self.processed_dir:\n",
    "            preprocessed_path = self.data_dir / self.processed_dir\n",
    "            if preprocessed_path.exists() and len(list(preprocessed_path.glob(\"*.png\"))) > 0:\n",
    "                use_preprocessed = True\n",
    "                print(f\"âœ… Using preprocessed images from {preprocessed_path}\")\n",
    "            else:\n",
    "                print(\"â„¹ï¸ Preprocessed images not found, will process on-the-fly\")\n",
    "        \n",
    "        # Create datasets\n",
    "        self.train_dataset = APTOSDataset(\n",
    "            dataframe=self.train_df,\n",
    "            image_dir=self.data_dir / self.image_dir,\n",
    "            transform=get_train_transforms(self.image_size),\n",
    "            preprocessor=preprocessor if not use_preprocessed else None,\n",
    "            use_preprocessed=use_preprocessed,\n",
    "            preprocessed_dir=preprocessed_path,\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = APTOSDataset(\n",
    "            dataframe=self.val_df,\n",
    "            image_dir=self.data_dir / self.image_dir,\n",
    "            transform=get_val_transforms(self.image_size),\n",
    "            preprocessor=preprocessor if not use_preprocessed else None,\n",
    "            use_preprocessed=use_preprocessed,\n",
    "            preprocessed_dir=preprocessed_path,\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ“Š Train samples: {len(self.train_dataset)}, Val samples: {len(self.val_dataset)}\")\n",
    "        print(f\"âš¡ GPU Optimizations: pin_memory={self.pin_memory}, persistent_workers={self.persistent_workers}, prefetch_factor={self.prefetch_factor}\")\n",
    "    \n",
    "    def _get_sampler(self):\n",
    "        if not self.use_weighted_sampler:\n",
    "            return None\n",
    "        \n",
    "        train_labels = self.train_df['diagnosis'].values\n",
    "        sample_weights = self.class_weights[train_labels]\n",
    "        \n",
    "        return WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        sampler = self._get_sampler()\n",
    "        loader_kwargs = {\n",
    "            'batch_size': self.batch_size,\n",
    "            'shuffle': (sampler is None),\n",
    "            'sampler': sampler,\n",
    "            'num_workers': self.num_workers,\n",
    "            'pin_memory': self.pin_memory,\n",
    "            'drop_last': True,\n",
    "        }\n",
    "        # Only add these if num_workers > 0\n",
    "        if self.num_workers > 0:\n",
    "            loader_kwargs['persistent_workers'] = self.persistent_workers\n",
    "            loader_kwargs['prefetch_factor'] = self.prefetch_factor\n",
    "        \n",
    "        return DataLoader(self.train_dataset, **loader_kwargs)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        loader_kwargs = {\n",
    "            'batch_size': self.batch_size * 2,  # Larger batch for validation (no gradients)\n",
    "            'shuffle': False,\n",
    "            'num_workers': self.num_workers,\n",
    "            'pin_memory': self.pin_memory,\n",
    "        }\n",
    "        # Only add these if num_workers > 0\n",
    "        if self.num_workers > 0:\n",
    "            loader_kwargs['persistent_workers'] = self.persistent_workers\n",
    "            loader_kwargs['prefetch_factor'] = self.prefetch_factor\n",
    "        \n",
    "        return DataLoader(self.val_dataset, **loader_kwargs)\n",
    "\n",
    "\n",
    "print(\"âœ… APTOSDataModule defined with GPU optimizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101c8b1",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ðŸ“Š Metrics & Threshold Optimization\n",
    "\n",
    "The primary metric is **Quadratic Weighted Kappa (QWK)** - measures agreement between predicted and actual grades with quadratic penalty for distance.\n",
    "\n",
    "Key technique: **Threshold Optimization** - find optimal thresholds to convert regression output to discrete classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd40130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Metrics and ThresholdOptimizer defined!\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "def quadratic_weighted_kappa(y_true: np.ndarray, y_pred: np.ndarray, num_classes: int = 5) -> float:\n",
    "    \"\"\"Calculate Quadratic Weighted Kappa.\"\"\"\n",
    "    y_true = np.clip(np.asarray(y_true).astype(int), 0, num_classes - 1)\n",
    "    y_pred = np.clip(np.asarray(y_pred).astype(int), 0, num_classes - 1)\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "\n",
    "# Threshold Optimizer\n",
    "class ThresholdOptimizer:\n",
    "    \"\"\"Optimize thresholds to maximize QWK.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 5, initial_thresholds: List[float] = None):\n",
    "        self.num_classes = num_classes\n",
    "        self.initial_thresholds = initial_thresholds or [0.5, 1.5, 2.5, 3.5]\n",
    "    \n",
    "    def _apply_thresholds(self, predictions: np.ndarray, thresholds: List[float]) -> np.ndarray:\n",
    "        \"\"\"Convert continuous predictions to classes.\"\"\"\n",
    "        classes = np.zeros(len(predictions), dtype=int)\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            classes[predictions > threshold] = i + 1\n",
    "        return classes\n",
    "    \n",
    "    def _objective(self, thresholds: np.ndarray, predictions: np.ndarray, targets: np.ndarray) -> float:\n",
    "        \"\"\"Objective: negative QWK (for minimization).\"\"\"\n",
    "        thresholds = np.sort(thresholds)\n",
    "        pred_classes = self._apply_thresholds(predictions, thresholds)\n",
    "        return -quadratic_weighted_kappa(targets, pred_classes, self.num_classes)\n",
    "    \n",
    "    def optimize(self, predictions: np.ndarray, targets: np.ndarray, verbose: bool = True) -> Tuple[List[float], float]:\n",
    "        \"\"\"Find optimal thresholds.\"\"\"\n",
    "        predictions = predictions.flatten()\n",
    "        targets = targets.flatten().astype(int)\n",
    "        \n",
    "        # Initial QWK\n",
    "        initial_qwk = -self._objective(np.array(self.initial_thresholds), predictions, targets)\n",
    "        if verbose:\n",
    "            print(f\"Initial QWK with thresholds {self.initial_thresholds}: {initial_qwk:.4f}\")\n",
    "        \n",
    "        # Optimize\n",
    "        result = minimize(\n",
    "            self._objective,\n",
    "            x0=np.array(self.initial_thresholds),\n",
    "            args=(predictions, targets),\n",
    "            method='Nelder-Mead',\n",
    "            options={'maxiter': 1000}\n",
    "        )\n",
    "        \n",
    "        optimized_thresholds = sorted(result.x.tolist())\n",
    "        best_qwk = -result.fun\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Optimized QWK: {best_qwk:.4f}\")\n",
    "            print(f\"Optimized thresholds: {[f'{t:.3f}' for t in optimized_thresholds]}\")\n",
    "        \n",
    "        return optimized_thresholds, best_qwk\n",
    "    \n",
    "    def apply(self, predictions: np.ndarray, thresholds: List[float] = None) -> np.ndarray:\n",
    "        \"\"\"Apply thresholds to predictions.\"\"\"\n",
    "        thresholds = thresholds or self.initial_thresholds\n",
    "        return self._apply_thresholds(predictions.flatten(), thresholds)\n",
    "\n",
    "\n",
    "print(\"âœ… Metrics and ThresholdOptimizer defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934e19d",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. âš¡ PyTorch Lightning Training Module\n",
    "\n",
    "The training module handles:\n",
    "- Forward pass and loss computation\n",
    "- Metric logging (train/val loss, QWK)\n",
    "- Optimizer and scheduler configuration\n",
    "- Threshold optimization at epoch end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604efcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DRTrainingModule defined!\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Lightning Training Module\n",
    "class DRTrainingModule(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning module for DR detection training.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str = \"tf_efficientnet_b5_ns\",\n",
    "        pretrained: bool = True,\n",
    "        learning_rate: float = 1e-4,\n",
    "        weight_decay: float = 1e-5,\n",
    "        num_epochs: int = 30,\n",
    "        optimize_thresholds: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Model\n",
    "        self.model = DRModel(\n",
    "            backbone_name=backbone_name,\n",
    "            pretrained=pretrained,\n",
    "            pooling=\"gem\",\n",
    "            head_dropout=0.5,\n",
    "        )\n",
    "        \n",
    "        # Loss\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_epochs = num_epochs\n",
    "        self.optimize_thresholds = optimize_thresholds\n",
    "        \n",
    "        # Threshold optimizer\n",
    "        self.threshold_optimizer = ThresholdOptimizer()\n",
    "        self.optimized_thresholds = [0.5, 1.5, 2.5, 3.5]\n",
    "        \n",
    "        # Store outputs for epoch-level metrics\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch['image']\n",
    "        targets = batch['target']\n",
    "        \n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        \n",
    "        # Store for epoch metrics\n",
    "        preds = outputs.round().clamp(0, 4).long()\n",
    "        self.training_step_outputs.append({\n",
    "            'loss': loss.detach(),\n",
    "            'preds': preds.detach().cpu(),\n",
    "            'targets': targets.long().detach().cpu(),\n",
    "        })\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        if self.training_step_outputs:\n",
    "            all_preds = torch.cat([x['preds'] for x in self.training_step_outputs])\n",
    "            all_targets = torch.cat([x['targets'] for x in self.training_step_outputs])\n",
    "            \n",
    "            qwk = quadratic_weighted_kappa(all_targets.numpy(), all_preds.numpy())\n",
    "            self.log('train_qwk', qwk, prog_bar=True)\n",
    "        \n",
    "        self.training_step_outputs.clear()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images = batch['image']\n",
    "        targets = batch['target']\n",
    "        \n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        \n",
    "        self.validation_step_outputs.append({\n",
    "            'loss': loss.detach(),\n",
    "            'outputs': outputs.detach().cpu(),\n",
    "            'targets': targets.long().detach().cpu(),\n",
    "        })\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.validation_step_outputs:\n",
    "            all_outputs = torch.cat([x['outputs'] for x in self.validation_step_outputs])\n",
    "            all_targets = torch.cat([x['targets'] for x in self.validation_step_outputs])\n",
    "            \n",
    "            # QWK with default thresholds\n",
    "            preds = all_outputs.round().clamp(0, 4).long()\n",
    "            qwk = quadratic_weighted_kappa(all_targets.numpy(), preds.numpy())\n",
    "            \n",
    "            # Optimize thresholds\n",
    "            if self.optimize_thresholds:\n",
    "                opt_thresholds, opt_qwk = self.threshold_optimizer.optimize(\n",
    "                    all_outputs.numpy(),\n",
    "                    all_targets.numpy(),\n",
    "                    verbose=False\n",
    "                )\n",
    "                self.optimized_thresholds = opt_thresholds\n",
    "                self.log('val_qwk_optimized', opt_qwk, prog_bar=True)\n",
    "            \n",
    "            self.log('val_qwk', qwk, prog_bar=True)\n",
    "            \n",
    "            # Accuracy\n",
    "            accuracy = (preds == all_targets).float().mean().item()\n",
    "            self.log('val_accuracy', accuracy, prog_bar=True)\n",
    "        \n",
    "        self.validation_step_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.num_epochs,\n",
    "            eta_min=self.learning_rate * 0.01,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {'scheduler': scheduler, 'interval': 'epoch'}\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ… DRTrainingModule defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b68fc1",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ðŸš€ Training\n",
    "\n",
    "Now let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33ca6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/data/aptos/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3914615464.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Setup datamodule (this loads and splits the data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nðŸ“Š DataModule ready!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2984356805.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Load CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Train/val split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/aptos/train.csv'"
     ]
    }
   ],
   "source": [
    "# Create DataModule with GPU optimizations\n",
    "datamodule = APTOSDataModule(\n",
    "    data_dir=config.DATA_DIR,\n",
    "    train_csv=config.TRAIN_CSV,\n",
    "    image_dir=config.TRAIN_IMAGES,\n",
    "    processed_dir=config.PROCESSED_IMAGES,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    image_size=config.IMAGE_SIZE,\n",
    "    val_split=config.VAL_SPLIT,\n",
    "    use_weighted_sampler=True,\n",
    "    use_preprocessing=config.USE_PREPROCESSING,\n",
    "    seed=config.SEED,\n",
    "    # GPU optimization parameters\n",
    "    pin_memory=config.PIN_MEMORY,\n",
    "    persistent_workers=config.PERSISTENT_WORKERS,\n",
    "    prefetch_factor=config.PREFETCH_FACTOR,\n",
    ")\n",
    "\n",
    "# Setup datamodule (this loads and splits the data)\n",
    "datamodule.setup()\n",
    "\n",
    "print(f\"\\nðŸ“Š DataModule ready!\")\n",
    "print(f\"   Train batches: {len(datamodule.train_dataloader())}\")\n",
    "print(f\"   Val batches: {len(datamodule.val_dataloader())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "images = batch['image']\n",
    "targets = batch['target']\n",
    "\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Targets: {targets}\")\n",
    "\n",
    "# Denormalize and show\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(images):\n",
    "        img = images[i].cpu() * std + mean\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Grade: {int(targets[i])} ({config.CLASS_NAMES[int(targets[i])]})\")\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137288cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lightning Module\n",
    "model = DRTrainingModule(\n",
    "    backbone_name=config.BACKBONE,\n",
    "    pretrained=True,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    num_epochs=config.EPOCHS,\n",
    "    optimize_thresholds=True,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Summary:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad88d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Warmup and Memory Check\n",
    "if torch.cuda.is_available():\n",
    "    print(\"ðŸ”¥ Warming up GPU...\")\n",
    "    \n",
    "    # Show GPU info\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"   GPU: {gpu_name}\")\n",
    "    print(f\"   Total Memory: {gpu_memory:.2f} GB\")\n",
    "    \n",
    "    # Warmup: run a few forward passes to initialize CUDA kernels\n",
    "    dummy_input = torch.randn(4, 3, config.IMAGE_SIZE, config.IMAGE_SIZE).cuda()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            _ = torch.nn.functional.conv2d(dummy_input, torch.randn(64, 3, 7, 7).cuda())\n",
    "    \n",
    "    del dummy_input\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Memory after warmup\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"   Memory Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Memory Reserved: {reserved:.2f} GB\")\n",
    "    print(\"âœ… GPU warmup complete!\")\n",
    "else:\n",
    "    print(\"âš ï¸ CUDA not available - running on CPU (will be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_dir = config.CHECKPOINT_DIR / timestamp\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename=\"dr-{epoch:02d}-{val_qwk:.4f}\",\n",
    "        monitor=\"val_qwk\",\n",
    "        mode=\"max\",\n",
    "        save_top_k=3,\n",
    "        save_last=True,\n",
    "        verbose=True,\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_qwk\",\n",
    "        mode=\"max\",\n",
    "        patience=config.PATIENCE,\n",
    "        verbose=True,\n",
    "    ),\n",
    "    LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "]\n",
    "\n",
    "# Loggers\n",
    "loggers = [\n",
    "    TensorBoardLogger(save_dir=config.LOG_DIR, name=\"tensorboard\"),\n",
    "    CSVLogger(save_dir=config.LOG_DIR, name=\"csv_logs\"),\n",
    "]\n",
    "\n",
    "print(f\"âœ… Callbacks and loggers configured!\")\n",
    "print(f\"   Checkpoints will be saved to: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550edde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer with GPU optimizations\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config.EPOCHS,\n",
    "    accelerator=\"auto\",  # Automatically detect GPU/CPU\n",
    "    devices=1,\n",
    "    precision=config.PRECISION,  # 16-mixed for faster training\n",
    "    callbacks=callbacks,\n",
    "    logger=loggers,\n",
    "    accumulate_grad_batches=config.ACCUMULATE_GRAD,\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    "    deterministic=False,  # False for speed (cuDNN benchmark enabled)\n",
    "    benchmark=True,  # Enable cuDNN auto-tuner for faster convolutions\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer configured!\")\n",
    "print(f\"   Max epochs: {config.EPOCHS}\")\n",
    "print(f\"   Precision: {config.PRECISION}\")\n",
    "print(f\"   Accumulate gradients: {config.ACCUMULATE_GRAD}\")\n",
    "print(f\"   Effective batch size: {config.BATCH_SIZE * config.ACCUMULATE_GRAD}\")\n",
    "print(f\"   cuDNN benchmark: Enabled\")\n",
    "print(f\"   Accelerator: {trainer.accelerator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c20038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(\"ðŸ§¹ GPU cache cleared - ready for training!\")\n",
    "    \n",
    "    # Display available memory\n",
    "    free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)\n",
    "    print(f\"   Available GPU memory: {free_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5724ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ START TRAINING\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸš€ STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE} (effective: {config.BATCH_SIZE * config.ACCUMULATE_GRAD})\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"Backbone: {config.BACKBONE}\")\n",
    "print(f\"Mixed Precision: {config.PRECISION}\")\n",
    "print(f\"Workers: {config.NUM_WORKERS} (pin_memory={config.PIN_MEMORY})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(f\"â±ï¸ Total training time: {training_time/60:.2f} minutes\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f254c",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. ðŸ“ˆ Evaluation & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdeeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Results\n",
    "print(\"ðŸ“Š Training Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best model info\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_qwk = trainer.checkpoint_callback.best_model_score\n",
    "\n",
    "print(f\"Best validation QWK: {best_qwk:.4f}\")\n",
    "print(f\"Best model saved to: {best_model_path}\")\n",
    "print(f\"Optimized thresholds: {model.optimized_thresholds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf324356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "best_model = DRTrainingModule.load_from_checkpoint(best_model_path)\n",
    "best_model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "best_model.to(device)\n",
    "\n",
    "# Run validation with GPU optimizations\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_outputs = []\n",
    "\n",
    "with torch.no_grad(), torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "    for batch in tqdm(datamodule.val_dataloader(), desc=\"Evaluating\"):\n",
    "        # Use non_blocking=True for async GPU transfer when using pinned memory\n",
    "        images = batch['image'].to(device, non_blocking=True)\n",
    "        targets = batch['target']\n",
    "        \n",
    "        outputs = best_model(images)\n",
    "        \n",
    "        all_outputs.extend(outputs.cpu().numpy())\n",
    "        all_targets.extend(targets.numpy())\n",
    "\n",
    "all_outputs = np.array(all_outputs)\n",
    "all_targets = np.array(all_targets).astype(int)\n",
    "\n",
    "# Apply optimized thresholds\n",
    "threshold_optimizer = ThresholdOptimizer()\n",
    "opt_thresholds, opt_qwk = threshold_optimizer.optimize(all_outputs, all_targets)\n",
    "all_preds = threshold_optimizer.apply(all_outputs, opt_thresholds)\n",
    "\n",
    "print(f\"\\nâœ… Final Validation QWK: {opt_qwk:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=config.CLASS_NAMES, yticklabels=config.CLASS_NAMES)\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=config.CLASS_NAMES, yticklabels=config.CLASS_NAMES)\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Normalized Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=config.CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history (from CSV logs)\n",
    "csv_log_dir = config.LOG_DIR / \"csv_logs\"\n",
    "versions = sorted(csv_log_dir.glob(\"version_*\"))\n",
    "\n",
    "if versions:\n",
    "    latest_version = versions[-1]\n",
    "    metrics_file = latest_version / \"metrics.csv\"\n",
    "    \n",
    "    if metrics_file.exists():\n",
    "        metrics_df = pd.read_csv(metrics_file)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Loss\n",
    "        if 'train_loss_epoch' in metrics_df.columns:\n",
    "            train_loss = metrics_df.dropna(subset=['train_loss_epoch'])\n",
    "            axes[0].plot(train_loss['epoch'], train_loss['train_loss_epoch'], label='Train', marker='o')\n",
    "        if 'val_loss' in metrics_df.columns:\n",
    "            val_loss = metrics_df.dropna(subset=['val_loss'])\n",
    "            axes[0].plot(val_loss['epoch'], val_loss['val_loss'], label='Validation', marker='o')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training & Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # QWK\n",
    "        if 'train_qwk' in metrics_df.columns:\n",
    "            train_qwk = metrics_df.dropna(subset=['train_qwk'])\n",
    "            axes[1].plot(train_qwk['epoch'], train_qwk['train_qwk'], label='Train', marker='o')\n",
    "        if 'val_qwk' in metrics_df.columns:\n",
    "            val_qwk = metrics_df.dropna(subset=['val_qwk'])\n",
    "            axes[1].plot(val_qwk['epoch'], val_qwk['val_qwk'], label='Validation', marker='o')\n",
    "        if 'val_qwk_optimized' in metrics_df.columns:\n",
    "            val_qwk_opt = metrics_df.dropna(subset=['val_qwk_optimized'])\n",
    "            axes[1].plot(val_qwk_opt['epoch'], val_qwk_opt['val_qwk_optimized'], \n",
    "                        label='Val (optimized)', marker='s', linestyle='--')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('QWK')\n",
    "        axes[1].set_title('Quadratic Weighted Kappa')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate\n",
    "        if 'lr-AdamW' in metrics_df.columns:\n",
    "            lr_data = metrics_df.dropna(subset=['lr-AdamW'])\n",
    "            axes[2].plot(lr_data['epoch'], lr_data['lr-AdamW'], marker='o', color='green')\n",
    "        axes[2].set_xlabel('Epoch')\n",
    "        axes[2].set_ylabel('Learning Rate')\n",
    "        axes[2].set_title('Learning Rate Schedule')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ No training logs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a73e2b",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. ðŸ’¾ Save & Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model info for inference\n",
    "model_info = {\n",
    "    'backbone': config.BACKBONE,\n",
    "    'image_size': config.IMAGE_SIZE,\n",
    "    'num_classes': config.NUM_CLASSES,\n",
    "    'optimized_thresholds': opt_thresholds,\n",
    "    'best_qwk': float(opt_qwk),\n",
    "    'checkpoint_path': str(best_model_path),\n",
    "}\n",
    "\n",
    "import json\n",
    "info_path = checkpoint_dir / 'model_info.json'\n",
    "with open(info_path, 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Model info saved to: {info_path}\")\n",
    "print(json.dumps(model_info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423dc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX (optional)\n",
    "def export_to_onnx(model, output_path, image_size=456):\n",
    "    \"\"\"Export model to ONNX format.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    dummy_input = torch.randn(1, 3, image_size, image_size).to(device)\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model.model,  # The actual model (not Lightning module)\n",
    "        dummy_input,\n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "    )\n",
    "    print(f\"âœ… ONNX model exported to: {output_path}\")\n",
    "\n",
    "# Uncomment to export\n",
    "# onnx_path = checkpoint_dir / 'model.onnx'\n",
    "# export_to_onnx(best_model, onnx_path, config.IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model files (for Colab)\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Create zip of checkpoint directory\n",
    "    import shutil\n",
    "    zip_path = f'/content/dr_model_{timestamp}.zip'\n",
    "    shutil.make_archive(zip_path.replace('.zip', ''), 'zip', checkpoint_dir)\n",
    "    \n",
    "    print(f\"ðŸ“¦ Model zip created: {zip_path}\")\n",
    "    print(\"Click below to download:\")\n",
    "    files.download(zip_path)\n",
    "else:\n",
    "    print(f\"ðŸ“ Model saved locally at: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447aca33",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. ðŸ”® Inference Demo\n",
    "\n",
    "Quick demonstration of how to use the trained model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_single_image(model, image_path, preprocessor, transform, thresholds, device='cuda'):\n",
    "    \"\"\"Predict DR grade for a single image.\"\"\"\n",
    "    # Load and preprocess\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Ben Graham preprocessing\n",
    "    processed = preprocessor(img)\n",
    "    processed_rgb = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Transform\n",
    "    transformed = transform(image=processed_rgb)\n",
    "    tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor).cpu().numpy()[0]\n",
    "    \n",
    "    # Apply thresholds\n",
    "    pred_class = 0\n",
    "    for i, t in enumerate(thresholds):\n",
    "        if output > t:\n",
    "            pred_class = i + 1\n",
    "    \n",
    "    return {\n",
    "        'raw_output': float(output),\n",
    "        'predicted_class': pred_class,\n",
    "        'class_name': config.CLASS_NAMES[pred_class],\n",
    "        'original_image': img_rgb,\n",
    "        'processed_image': processed_rgb,\n",
    "    }\n",
    "\n",
    "print(\"âœ… Inference function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: predict on validation samples\n",
    "preprocessor = BenGrahamPreprocessor(output_size=512)\n",
    "transform = get_val_transforms(config.IMAGE_SIZE)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Get some samples from each class\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 25))\n",
    "\n",
    "for class_idx in range(5):\n",
    "    # Get a validation sample from this class\n",
    "    val_samples = datamodule.val_df[datamodule.val_df['diagnosis'] == class_idx]\n",
    "    if len(val_samples) > 0:\n",
    "        sample = val_samples.iloc[0]\n",
    "        img_name = sample['id_code']\n",
    "        \n",
    "        # Find image\n",
    "        img_path = None\n",
    "        for ext in ['.png', '.jpg', '.jpeg']:\n",
    "            potential = config.DATA_DIR / config.TRAIN_IMAGES / f\"{img_name}{ext}\"\n",
    "            if potential.exists():\n",
    "                img_path = potential\n",
    "                break\n",
    "        \n",
    "        if img_path:\n",
    "            result = predict_single_image(\n",
    "                best_model, img_path, preprocessor, transform, \n",
    "                opt_thresholds, device\n",
    "            )\n",
    "            \n",
    "            # Original\n",
    "            axes[class_idx, 0].imshow(result['original_image'])\n",
    "            axes[class_idx, 0].set_title(f\"Original\\\\nTrue: {config.CLASS_NAMES[class_idx]}\")\n",
    "            axes[class_idx, 0].axis('off')\n",
    "            \n",
    "            # Processed\n",
    "            axes[class_idx, 1].imshow(result['processed_image'])\n",
    "            axes[class_idx, 1].set_title(f\"Processed\")\n",
    "            axes[class_idx, 1].axis('off')\n",
    "            \n",
    "            # Prediction\n",
    "            color = 'green' if result['predicted_class'] == class_idx else 'red'\n",
    "            axes[class_idx, 2].text(0.5, 0.6, f\"Predicted: {result['class_name']}\", \n",
    "                                    ha='center', va='center', fontsize=16, color=color,\n",
    "                                    transform=axes[class_idx, 2].transAxes)\n",
    "            axes[class_idx, 2].text(0.5, 0.4, f\"Raw: {result['raw_output']:.3f}\", \n",
    "                                    ha='center', va='center', fontsize=12,\n",
    "                                    transform=axes[class_idx, 2].transAxes)\n",
    "            axes[class_idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853d7c0",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“ Summary\n",
    "\n",
    "This notebook provides a complete end-to-end pipeline for Diabetic Retinopathy detection:\n",
    "\n",
    "### What was implemented:\n",
    "1. **Data Loading** - APTOS 2019 dataset with visualization\n",
    "2. **Ben Graham Preprocessing** - Industry-standard fundus image preprocessing\n",
    "3. **Model Architecture** - EfficientNet-B5 with GeM pooling and regression head\n",
    "4. **Data Augmentation** - Rotation-invariant augmentations for fundus images\n",
    "5. **Training** - PyTorch Lightning with early stopping and checkpointing\n",
    "6. **Threshold Optimization** - Maximize QWK by finding optimal thresholds\n",
    "7. **Evaluation** - Confusion matrix and classification report\n",
    "8. **Export** - Save model for deployment\n",
    "\n",
    "### Key Techniques:\n",
    "- **Regression approach** instead of classification (ordinal nature of DR grades)\n",
    "- **Threshold optimization** to convert regression to classes\n",
    "- **Weighted sampling** to handle class imbalance\n",
    "- **GeM Pooling** for better feature aggregation\n",
    "- **Cosine annealing** learning rate schedule\n",
    "\n",
    "### Next Steps:\n",
    "- Try different backbones (EfficientNet-B6, B7)\n",
    "- Implement Test-Time Augmentation (TTA)\n",
    "- Add Grad-CAM visualization for explainability\n",
    "- Fine-tune on additional datasets (EyePACS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
